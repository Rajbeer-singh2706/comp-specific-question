{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8de63b6",
   "metadata": {},
   "source": [
    "Nice — here’s a **production-ready PySpark template** for a PB-scale historical backfill. It’s designed to be practical and interview-friendly: chunked by partitions (year/month), checkpointed via a control (status) Delta table, tolerant to failures, supports schema evolution, dedup/upsert logic, and post-load optimization (file compaction / Z-Order).\n",
    "\n",
    "Copy-paste & adapt to your environment (S3/ADLS/HDFS, cluster size, and Delta/Parquet specifics).\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "PySpark production template: Historical backfill into Delta (partitioned, checkpointed, resumable)\n",
    "\n",
    "Assumptions:\n",
    "- Spark session (spark) is already created (on Databricks / EMR / Yarn).\n",
    "- Delta Lake is available on the cluster.\n",
    "- Raw files are stored in S3/ADLS/HDFS under a structured path: s3://bucket/raw/year=YYYY/month=MM/\n",
    "- We maintain a control Delta table to track completed partitions.\n",
    "- We write to a partitioned Delta table: s3://bucket/curated/table/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565727ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# ------------- Configurations -------------\n",
    "RAW_BASE_PATH = \"s3://bucket/raw/\"\n",
    "DELTA_TARGET_PATH = \"s3://bucket/curated/table/\"\n",
    "CONTROL_TABLE_PATH = \"s3://bucket/curated/_control/historical_load_status/\"  # Delta table to track progress\n",
    "# Logical table name for SQL catalog if you want:\n",
    "TARGET_TABLE_NAME = \"curated.transactions\"\n",
    "\n",
    "# Partitioning strategy for backfill (year, month)\n",
    "START_YEAR = 2018\n",
    "END_YEAR = 2023\n",
    "\n",
    "# Performance tuning (tweak as per cluster)\n",
    "SPARK_CONF = {\n",
    "    \"spark.sql.shuffle.partitions\": \"2000\",\n",
    "    \"spark.default.parallelism\": \"2000\",\n",
    "    \"spark.sql.adaptive.enabled\": \"true\",\n",
    "    \"spark.databricks.delta.optimizeWriter.enabled\": \"true\"  # Databricks OPTIMIZE helpers (if available)\n",
    "}\n",
    "\n",
    "# Read options\n",
    "READ_OPTIONS = {\n",
    "    \"multiLine\": True,\n",
    "    \"mode\": \"PERMISSIVE\",         # or FAILFAST / DROPMALFORMED\n",
    "    \"inferSchema\": False          # Prefer explicit schema in prod for stability; set True for quick dev\n",
    "}\n",
    "\n",
    "# Write options\n",
    "WRITE_MODE = \"append\"\n",
    "DELTA_WRITE_OPTIONS = {\n",
    "    \"mergeSchema\": \"true\",\n",
    "    # \"overwriteSchema\": \"true\"  # Only use carefully if you intend to replace schema\n",
    "}\n",
    "\n",
    "# Dedup/upsert keys (set to None if purely append)\n",
    "UPSERT_KEY = [\"transaction_id\"]    # use primary key(s) for upsert/dedup\n",
    "\n",
    "# Schema (preferred in prod). Example skeleton — replace with real schema.\n",
    "SCHEMA = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),    # redundant if partition columns exist; useful for queries\n",
    "    StructField(\"month\", IntegerType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# ------------- Initialize Spark -------------\n",
    "def init_spark():\n",
    "    builder = SparkSession.builder.appName(\"historical_backfill\")\n",
    "    for k, v in SPARK_CONF.items():\n",
    "        builder = builder.config(k, v)\n",
    "    spark = builder.getOrCreate()\n",
    "    # Optional: set shuffle partitions explicitly in runtime\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", SPARK_CONF[\"spark.sql.shuffle.partitions\"])\n",
    "    return spark\n",
    "\n",
    "\n",
    "# ------------- Control table helpers -------------\n",
    "def ensure_control_table(spark):\n",
    "    \"\"\"\n",
    "    Ensure the control table exists (Delta) to record status per partition.\n",
    "    Fields: year, month, status, start_ts, end_ts, message\n",
    "    \"\"\"\n",
    "    # If the path doesn't exist, create an empty DataFrame and write Delta\n",
    "    try:\n",
    "        spark.read.format(\"delta\").load(CONTROL_TABLE_PATH)\n",
    "    except Exception:\n",
    "        control_schema = StructType([\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "            StructField(\"month\", IntegerType(), True),\n",
    "            StructField(\"status\", StringType(), True),\n",
    "            StructField(\"start_ts\", TimestampType(), True),\n",
    "            StructField(\"end_ts\", TimestampType(), True),\n",
    "            StructField(\"message\", StringType(), True)\n",
    "        ])\n",
    "        empty_df = spark.createDataFrame([], control_schema)\n",
    "        empty_df.write.format(\"delta\").mode(\"overwrite\").save(CONTROL_TABLE_PATH)\n",
    "\n",
    "\n",
    "def is_partition_completed(spark, year, month):\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(CONTROL_TABLE_PATH)\n",
    "        recs = df.filter((col(\"year\") == year) & (col(\"month\") == month) & (col(\"status\") == \"SUCCESS\")).count()\n",
    "        return recs > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def mark_partition_status(spark, year, month, status, message=None):\n",
    "    now_ts = datetime.utcnow()\n",
    "    row = spark.createDataFrame([(year, month, status, now_ts, now_ts, message or \"\")],\n",
    "                                [\"year\", \"month\", \"status\", \"start_ts\", \"end_ts\", \"message\"])\n",
    "    # Append status row (you can update logic to upsert latest status)\n",
    "    row.write.format(\"delta\").mode(\"append\").save(CONTROL_TABLE_PATH)\n",
    "\n",
    "\n",
    "# ------------- Read, transform, write logic -------------\n",
    "def process_partition(spark, year, month):\n",
    "    raw_path = f\"{RAW_BASE_PATH.rstrip('/')}/year={year}/month={str(month).zfill(2)}/\"\n",
    "    print(f\"[INFO] Starting processing for {year}-{month} from {raw_path}\")\n",
    "\n",
    "    # Mark as RUNNING\n",
    "    mark_partition_status(spark, year, month, \"RUNNING\", f\"Started partition {year}-{month}\")\n",
    "\n",
    "    try:\n",
    "        # Read\n",
    "        df = spark.read.format(\"json\").options(**READ_OPTIONS).schema(SCHEMA).load(raw_path)\n",
    "\n",
    "        # Quick validation / early filter to reduce volume (predicate pushdown)\n",
    "        # Example: Only keep records with non-null transaction_id\n",
    "        df = df.filter(col(\"transaction_id\").isNotNull())\n",
    "\n",
    "        # Basic transformations (example)\n",
    "        # - derive year/month from event_time if missing\n",
    "        # - normalize columns\n",
    "        df = df.withColumn(\"year\", col(\"year\")) \\\n",
    "               .withColumn(\"month\", col(\"month\"))\n",
    "\n",
    "        # Repartition for parallelism before expensive ops / write\n",
    "        # Choose number of partitions based on data volume; don't under/over partition\n",
    "        df = df.repartition(400, \"region\")  # tune 400 -> cluster dependent\n",
    "\n",
    "        # If upsert/dedup is needed, do a merge using Delta\n",
    "        if UPSERT_KEY:\n",
    "            # Write to a staging path for idempotent staging (optional)\n",
    "            staging_path = DELTA_TARGET_PATH.rstrip(\"/\") + \"/_staging/\" + f\"year={year}/month={str(month).zfill(2)}/\"\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").options(**DELTA_WRITE_OPTIONS).save(staging_path)\n",
    "\n",
    "            # Merge staging into target Delta table\n",
    "            # If target doesn't exist yet, create it by moving staging\n",
    "            try:\n",
    "                target_exists = spark.read.format(\"delta\").load(DELTA_TARGET_PATH) is not None\n",
    "                target_exists = True\n",
    "            except Exception:\n",
    "                target_exists = False\n",
    "\n",
    "            if not target_exists:\n",
    "                # create target by moving staging -> target (simple approach)\n",
    "                DeltaTable.forPath(spark, staging_path).toDF().write.format(\"delta\").mode(\"overwrite\").partitionBy(\"year\",\"month\").save(DELTA_TARGET_PATH)\n",
    "            else:\n",
    "                target = DeltaTable.forPath(spark, DELTA_TARGET_PATH)\n",
    "                staging = DeltaTable.forPath(spark, staging_path)\n",
    "\n",
    "                # Build merge condition on keys\n",
    "                cond = \" AND \".join([f\"target.{k} = source.{k}\" for k in UPSERT_KEY])\n",
    "                # Perform merge:\n",
    "                target.alias(\"target\").merge(\n",
    "                    staging.toDF().alias(\"source\"),\n",
    "                    cond\n",
    "                ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "            # Optional: cleanup staging\n",
    "            # dbutils.fs.rm(staging_path, recurse=True)  # Databricks helper; use fs commands accordingly\n",
    "        else:\n",
    "            # Pure append\n",
    "            df.write.format(\"delta\") \\\n",
    "              .mode(WRITE_MODE) \\\n",
    "              .options(**DELTA_WRITE_OPTIONS) \\\n",
    "              .partitionBy(\"year\", \"month\") \\\n",
    "              .save(DELTA_TARGET_PATH)\n",
    "\n",
    "        # Mark SUCCESS\n",
    "        mark_partition_status(spark, year, month, \"SUCCESS\", \"Completed successfully\")\n",
    "        print(f\"[INFO] Completed processing for {year}-{month}\")\n",
    "    except Exception as e:\n",
    "        # Mark FAILURE with message\n",
    "        mark_partition_status(spark, year, month, \"FAILED\", f\"Error: {str(e)}\")\n",
    "        print(f\"[ERROR] Processing failed for {year}-{month}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# ------------- Main orchestration -------------\n",
    "def run_backfill():\n",
    "    spark = init_spark()\n",
    "    ensure_control_table(spark)\n",
    "\n",
    "    # Generate partitions to process (example: year/month)\n",
    "    partitions = []\n",
    "    for y in range(START_YEAR, END_YEAR + 1):\n",
    "        months = range(1, 13)\n",
    "        # If start/end month constraints exist, handle them here\n",
    "        for m in months:\n",
    "            partitions.append((y, m))\n",
    "\n",
    "    # Optionally parallelize partition execution using a lightweight orchestration service (Airflow)\n",
    "    # Here we run sequentially, but can be parallelized by chunking partitions and running multiple jobs\n",
    "    for year, month in partitions:\n",
    "        # if already completed, skip\n",
    "        if is_partition_completed(spark, year, month):\n",
    "            print(f\"[INFO] Skipping completed partition {year}-{month}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            process_partition(spark, year, month)\n",
    "        except Exception as e:\n",
    "            # Depending on SLA, we can continue or abort; here we continue but log failure\n",
    "            print(f\"[WARN] Partition {year}-{month} failed. Continuing with next partition. Error: {e}\")\n",
    "            # Optionally: escalate/notify alerting system here\n",
    "\n",
    "    # Post-load maintenance: file compaction & optimization\n",
    "    try:\n",
    "        # Databricks: OPTIMIZE + ZORDER (useful for large tables)\n",
    "        # Example: optimize by region and date for locality\n",
    "        try:\n",
    "            spark.sql(f\"OPTIMIZE delta.`{DELTA_TARGET_PATH}` ZORDER BY (region, year, month)\")\n",
    "        except Exception:\n",
    "            print(\"[WARN] OPTIMIZE failed or not available in this environment. You can run compaction via other means.\")\n",
    "\n",
    "        # Vacuum - keep retention window large enough to avoid removing recent versions\n",
    "        try:\n",
    "            spark.sql(f\"VACUUM delta.`{DELTA_TARGET_PATH}` RETAIN 168 HOURS\")\n",
    "        except Exception:\n",
    "            print(\"[WARN] VACUUM failed or not available in this environment.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Post-load optimization had issues: {e}\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_backfill()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a41250",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Key points & reasoning (what to mention in an interview)\n",
    "\n",
    "* **Chunking**: Process by year/month (or logical shards) — avoid loading PB in one job.\n",
    "* **Checkpointing**: Use a control Delta table to record status for each partition — enables resume and idempotence.\n",
    "* **Schema**: Prefer explicit `StructType` in production. Use `mergeSchema` carefully; explicit schema reduces surprises.\n",
    "* **Parallelism**: Repartition by a high-cardinality column (e.g., `region`) and tune `spark.sql.shuffle.partitions`.\n",
    "* **Upsert vs Append**: For historical loads you may need dedupe/upsert — use Delta `MERGE`. Staging + merge pattern is safer and idempotent.\n",
    "* **Staging path**: Write staging partitions then merge — helps with idempotency and rollback.\n",
    "* **File compaction**: Run `OPTIMIZE`/`ZORDER` and `VACUUM` after full load to de-duplicate small files and speed queries.\n",
    "* **Monitoring & Observability**: Emit metrics, record errors in control table, and integrate with alerting (CloudWatch/Datadog).\n",
    "* **Cost & cluster**: Use spot instances or transient clusters for heavy backfills; autoscale to reduce cost.\n",
    "* **Testing**: Smoke test on a small sample, then ramp partitions (e.g., week → month → year), check job stability and correctness.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can:\n",
    "\n",
    "* produce a **short 2-minute spoken answer** you can deliver in interviews,\n",
    "* adapt the code to show **Airflow DAG** orchestration for parallel partition runs,\n",
    "* or convert this into a **notebook-ready** demo with sample data and unit tests.\n",
    "\n",
    "Which one next?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
