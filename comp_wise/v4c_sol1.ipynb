{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97d0e39",
   "metadata": {},
   "source": [
    "## 1) **SQL — Find 2nd highest salary per department**\n",
    "\n",
    "Problem: table `employees(emp_id, dept, salary)`. Return highest and 2nd highest salary per `dept`.\n",
    "Solution (standard, window approach):\n",
    "\n",
    "```sql\n",
    "SELECT dept, salary\n",
    "FROM (\n",
    "  SELECT dept, salary,\n",
    "         ROW_NUMBER() OVER (PARTITION BY dept ORDER BY salary DESC) AS rn\n",
    "  FROM employees\n",
    ") t\n",
    "WHERE rn = 2;\n",
    "```\n",
    "\n",
    "Notes: use `DENSE_RANK()` instead of `ROW_NUMBER()` if you want ties to be handled (i.e., second distinct salary).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 2) **Spark / PySpark — remove duplicates and write partitioned Delta**\n",
    "\n",
    "Task: read large JSON/Parquet, dedupe by `id`, write to Delta partitioned by `year` and `month` with schema enforcement.\n",
    "\n",
    "PySpark snippet:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month\n",
    "\n",
    "spark = SparkSession.builder.appName(\"dedupe_write\").getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(\"s3://bucket/path/\")  # or json/csv\n",
    "# assume df has timestamp column 'ts' and unique id 'id'\n",
    "df = df.withColumn(\"year\", year(col(\"ts\"))).withColumn(\"month\", month(col(\"ts\")))\n",
    "\n",
    "# dedupe keeping latest record per id using window\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc\n",
    "\n",
    "w = Window.partitionBy(\"id\").orderBy(desc(\"ts\"))\n",
    "df_dedup = df.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "(df_dedup\n",
    "  .write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"overwrite\")              # or \"append\" depending on use-case\n",
    "  .partitionBy(\"year\",\"month\")\n",
    "  .option(\"overwriteSchema\",\"true\")\n",
    "  .save(\"/mnt/delta/table_name\"))\n",
    "```\n",
    "\n",
    "# Optimization hints: repartition by partition keys before write to avoid small files; use `coalesce`/`repartition` tuned to executor cores; enable dynamic partition overwrite if \n",
    "# doing incremental partition updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f62caf",
   "metadata": {},
   "source": [
    "## 3) **System design / pipeline question — design ingestion for PB-scale historical + streaming**\n",
    "\n",
    "Short bullets you can speak to in an interview:\n",
    "\n",
    "* Bronze (raw) landing: Parquet/avro on S3 with time-based partitions.\n",
    "* Ingestion: bulk backfill via Spark clusters (EMR/Databricks) using parallel reads; streaming via Kafka → Structured Streaming.\n",
    "* Processing: Use Delta Lake for ACID, schema evolution; apply schema enforcement and compaction (OPTIMIZE / ZORDER).\n",
    "* Orchestration: Airflow / Databricks Jobs for batch; EventBridge/Lambda/Step Functions for triggers.\n",
    "* Cost/perf: auto-scaling clusters, spot instances for heavy batch; data skipping & partition pruning; cache hot tables.\n",
    "  (If you want, I can draw an architecture diagram for this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4) **PySpark code problem — count unique users per 1-hour window from Kafka**\n",
    "\n",
    "Core idea (Structured Streaming):\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import from_json, col, window\n",
    "schema = \"user_id STRING, event_time TIMESTAMP, action STRING\"\n",
    "\n",
    "raw = (spark.readStream.format(\"kafka\")\n",
    "       .option(\"kafka.bootstrap.servers\", \"host:port\")\n",
    "       .option(\"subscribe\", \"topic\")\n",
    "       .load())\n",
    "\n",
    "events = raw.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"j\")).select(\"j.*\")\n",
    "\n",
    "# watermark + window\n",
    "agg = (events\n",
    "       .withWatermark(\"event_time\", \"10 minutes\")\n",
    "       .groupBy(window(col(\"event_time\"), \"1 hour\"))\n",
    "       .agg(countDistinct(\"user_id\").alias(\"unique_users\")))\n",
    "\n",
    "query = (agg.writeStream.format(\"console\")   # or Delta sink\n",
    "         .outputMode(\"complete\")\n",
    "         .start())\n",
    "#Talk about watermarking, late data, and checkpointing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535ef37",
   "metadata": {},
   "source": [
    "## 5) **Performance tuning question — large join taking too long**\n",
    "\n",
    "Answer outline to give in interview:\n",
    "\n",
    "* Check skew: use `salting` or broadcast smaller table.\n",
    "* Use broadcast join if one table is small (`broadcast(df)`).\n",
    "* Repartition on join keys to avoid shuffle.\n",
    "* Persist intermediate results if reused.\n",
    "* Use column pruning and `select` only required columns.\n",
    "* Use proper file formats (Parquet/ORC) and partitioning; tune shuffle partitions (`spark.sql.shuffle.partitions`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11871d5a",
   "metadata": {},
   "source": [
    "## 6) **Behavioral / case question — explain a project where you improved cost or performance**\n",
    "\n",
    "Structure your answer: Situation → Task → Action → Result (quantify). Example quick bullet:\n",
    "\n",
    "* S: ETL job took 6 hours and cost $X/day.\n",
    "* T: Reduce runtime and cost.\n",
    "* A: Rewrote join strategy, reduced data scanned via partition pruning, switched to spot instances for batch.\n",
    "* R: Runtime dropped to 45 minutes, cost cut by 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f724c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd3f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
