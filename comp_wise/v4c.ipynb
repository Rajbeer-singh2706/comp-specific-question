{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help me understanding your migration project, Give me complete example of it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de0df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks \n",
    "# Q) Client wants to save data into unity catalog, but at the same time there is pii data , How do we use some feature of governace which are present inside unity catalog\n",
    "# Q) How to cetify the data?\n",
    "# Q) why client should use medalian architedure instead of dumping everything into unity cataliog vand then use it \n",
    "# Q) Lets see if u are working in databricks and u want to implement time travel , How to you implement time travel using delta table in databricks\n",
    "# Q) Assume client is asking you the diference between the serverless and any compute u create in databricks , how are u going to tell the difference to client, \n",
    "# What the benefits , which one to use?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### SQL ###########\n",
    "# for example there is sales tables assume some column\n",
    "# top 3 product based on revenu for partiular month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark  \n",
    "# In a technical cloud platform , how do you descibre the optimal data loading pipeline , do you prefer to fully transform the data into parquet/delta \n",
    "# or u want to do lighweigt transformation , offload it lighweight and then do the rest of transformation on delta tables?\n",
    "\n",
    "# Q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kafka\n",
    "# WHile ingested data  you using kafka connect, from external transational database to a datalake\n",
    "# What are options present with us \n",
    "# can u also elobrate the Fault Tolerance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
