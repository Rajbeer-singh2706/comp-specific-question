{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ad29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Q ) How data is ingested to your pipeline ?\n",
    "# Q)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strucrued Streaming \n",
    "# Q) What u do for late arrival of data in your pipeline \n",
    "# Q) What is trigger point of kafka , How frequenlty u poll data from Pipeline \n",
    "# Q) What is volune of data is being processed in your pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8648cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality \n",
    "# How data is managed in your pipeline \n",
    "# What types of Checked is done on streaming vs batch processing ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980bc6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark \n",
    "# What is Volumne of data is be processed \n",
    "# What is Bucketing \n",
    "# How do you deceding bucketing , For example customers(cust_id, cust_name, ..)\n",
    "# What is difference between partitioning vs bucketing \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d30852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestrate \n",
    "# Q) How did you orchestrate your pipeline ?\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb7e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL_Problem # or Pyspark \n",
    "'''\n",
    "Input_table\n",
    "-------------\n",
    "product,failure_date\n",
    "A , 10-AUG\n",
    "A , 11-AUG\n",
    "A , 12-AUG \n",
    "\n",
    "A , 14-AUG\n",
    "A , 15-AUG \n",
    "\n",
    "B , 10-AUG\n",
    "B , 11-AUG\n",
    "B , 12-AUG\n",
    "B , 13-AUG \n",
    "C , 10-AUG \n",
    "D , 11-AUG  \n",
    "\n",
    "\n",
    "Output \n",
    "--------\n",
    "product,min_date, max_date \n",
    "A, 10-AUG , 12-AUG \n",
    "A, 14-AUG , 15-AUG \n",
    "B, 10-AUG , 13-AUG \n",
    "C, 10-AUG , NA\n",
    "D, 11-AUG , NA \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b91371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09d444c7",
   "metadata": {},
   "source": [
    "## Pyspark Problem \n",
    "Got it ðŸ‘ â€” youâ€™re showing a **data transformation** problem often asked in PySpark interviews.\n",
    "\n",
    "You have **wide data** (columns `CL1`, `CL2`, `CL3`) and you need to **normalize** or **unpivot (melt)** it into a **long format** â€” i.e., transform columns into rows.\n",
    "\n",
    "Letâ€™s restate and solve step by step.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© **Input Data (Wide Format)**\n",
    "\n",
    "| ID | CL1 | CL2 | CL3 |\n",
    "| -- | --- | --- | --- |\n",
    "| A  | Y   |     |     |\n",
    "| B  | Y   | Y   |     |\n",
    "| C  | Y   | Y   | Y   |\n",
    "| D  |     | Y   |     |\n",
    "| E  |     |     |     |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ **Expected Output (Long Format)**\n",
    "\n",
    "| ID | CL |\n",
    "| -- | -- |\n",
    "| A  | 1  |\n",
    "| B  | 1  |\n",
    "| B  | 2  |\n",
    "| C  | 1  |\n",
    "| C  | 2  |\n",
    "| C  | 3  |\n",
    "| D  | 2  |\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ **PySpark Solution**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, when, array, explode, lit\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"UnpivotExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"A\", \"Y\", None, None),\n",
    "    (\"B\", \"Y\", \"Y\", None),\n",
    "    (\"C\", \"Y\", \"Y\", \"Y\"),\n",
    "    (\"D\", None, \"Y\", None),\n",
    "    (\"E\", None, None, None)\n",
    "]\n",
    "\n",
    "columns = [\"ID\", \"CL1\", \"CL2\", \"CL3\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"=== Original Data ===\")\n",
    "df.show()\n",
    "\n",
    "# Create an array of (column index) where value == 'Y'\n",
    "# Unpivot / melt logic using explode\n",
    "melted_df = (\n",
    "    df.withColumn(\n",
    "        \"CL_array\",\n",
    "        array(\n",
    "            when(df.CL1 == \"Y\", lit(1)),\n",
    "            when(df.CL2 == \"Y\", lit(2)),\n",
    "            when(df.CL3 == \"Y\", lit(3))\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"CL\", explode(expr(\"filter(CL_array, x -> x is not null)\")))\n",
    "    .select(\"ID\", \"CL\")\n",
    ")\n",
    "\n",
    "print(\"=== Transformed Data ===\")\n",
    "melted_df.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  **Explanation**\n",
    "\n",
    "| Step                 | What Happens                                                         |\n",
    "| -------------------- | -------------------------------------------------------------------- |\n",
    "| `array()`            | Combine CL1, CL2, CL3 into a single array of possible column indices |\n",
    "| `when()`             | Only keep column index if value is 'Y'                               |\n",
    "| `filter()`           | Remove nulls from the array                                          |\n",
    "| `explode()`          | Convert each element in array to a new row                           |\n",
    "| `select(\"ID\", \"CL\")` | Keep final columns                                                   |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Output**\n",
    "\n",
    "```\n",
    "+---+---+\n",
    "| ID| CL|\n",
    "+---+---+\n",
    "|  A|  1|\n",
    "|  B|  1|\n",
    "|  B|  2|\n",
    "|  C|  1|\n",
    "|  C|  2|\n",
    "|  C|  3|\n",
    "|  D|  2|\n",
    "+---+---+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ **Interview Tip:**\n",
    "\n",
    "This is called **unpivoting** or **normalizing** a DataFrame.\n",
    "\n",
    "In PySpark, thereâ€™s no direct `melt()` like in pandas, so you use:\n",
    "\n",
    "* `array()` + `explode()` for manual unpivot\n",
    "  or\n",
    "* SQL `UNION ALL` if columns are few (less dynamic but simpler).\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show the **SQL-style approach** as well (using `UNION ALL`) â€” itâ€™s simpler and interviewers often ask both versions for comparison?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1006ff7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
