{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For comapny specific I will keep in same folder only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on clearing Round 1! For Round 2, expect deeper technical questions and problem-solving scenarios. Hereâ€™s what to prepare:\n",
    "\n",
    "### **1. AWS Glue**\n",
    "- **How does AWS Glue work?**\n",
    "  - AWS Glue is a serverless ETL service that automates data preparation. It uses crawlers to detect schema and generates PySpark scripts for transformation.\n",
    "\n",
    "- **How do you optimize Glue jobs?**\n",
    "  - Use DynamicFrame over DataFrame for schema evolution.\n",
    "  - Enable job bookmarks for incremental processing.\n",
    "  - Partition data for parallelism.\n",
    "  - Use S3 storage classes wisely (e.g., Standard vs. Intelligent-Tiering).\n",
    "\n",
    "- **What are Glue job types?**\n",
    "  - **Spark ETL Jobs** (batch processing)\n",
    "  - **Streaming Jobs** (real-time ingestion)\n",
    "  - **Python Shell Jobs** (lighter workloads)\n",
    "\n",
    "- **Whatâ€™s the difference between Glue DynamicFrame and DataFrame?**\n",
    "  - **DynamicFrame:** Schema-aware, handles semi-structured data, supports AWS Glue transformations.\n",
    "  - **DataFrame:** Uses standard PySpark functions, better for performance tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. PySpark**\n",
    "- **How do you handle large datasets efficiently in PySpark?**\n",
    "  - Use **broadcast joins** for small lookup tables.\n",
    "  - Optimize **shuffle operations** (reduce unnecessary shuffling).\n",
    "  - Cache frequently accessed DataFrames.\n",
    "  - Set proper partitioning (`repartition()` vs `coalesce()`).\n",
    "\n",
    "- **Explain Window Functions in PySpark.**\n",
    "  - Used for ranking, aggregations, running totals, etc.\n",
    "  - Example:\n",
    "    ```python\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import row_number\n",
    "\n",
    "    window_spec = Window.partitionBy(\"category\").orderBy(\"price\")\n",
    "    df = df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "    ```\n",
    "\n",
    "- **How do you handle skewed data in PySpark?**\n",
    "  - **Salting:** Introduce random keys to balance partitions.\n",
    "  - **Bucketing:** Store data in predefined hash buckets.\n",
    "  - **Skew Join Handling:** Use `skewHint()` to redistribute load.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3. SQL**\n",
    "- **How do you optimize a slow-running SQL query?**\n",
    "  - Use indexes effectively.\n",
    "  - Avoid SELECT *; specify required columns.\n",
    "  - Use proper join strategies (HASH JOIN vs. MERGE JOIN).\n",
    "  - Partition large tables.\n",
    "  - Analyze query execution plans (`EXPLAIN`).\n",
    "\n",
    "- **Write a SQL query to get the second-highest salary.**\n",
    "  ```sql\n",
    "  SELECT MAX(salary) FROM employees \n",
    "  WHERE salary < (SELECT MAX(salary) FROM employees);\n",
    "  ```\n",
    "\n",
    "- **How do you handle duplicate records in SQL?**\n",
    "  - Using `DISTINCT`\n",
    "  - Using `GROUP BY`\n",
    "  - Using `ROW_NUMBER()` and deleting duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Python**\n",
    "- **How do you handle memory issues in Python when processing large datasets?**\n",
    "  - Use **generators** instead of lists.\n",
    "  - Process data in **chunks** using `pandas.read_csv(chunk_size=5000)`.\n",
    "  - Use `multiprocessing` for parallel processing.\n",
    "\n",
    "- **What is the difference between deep copy and shallow copy?**\n",
    "  - **Shallow Copy (`copy.copy()`):** Creates a new object but references the original nested objects.\n",
    "  - **Deep Copy (`copy.deepcopy()`):** Recursively copies all objects.\n",
    "\n",
    "- **How does Python manage memory?**\n",
    "  - Uses **Garbage Collection (GC)** with reference counting.\n",
    "  - `gc.collect()` manually triggers garbage collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Scenario-Based Questions**\n",
    "- **You have a Glue job processing daily 100GB CSV files. How do you optimize it?**\n",
    "  - Use **columnar formats** (Parquet/ORC) instead of CSV.\n",
    "  - Partition data properly.\n",
    "  - Use **pushdown predicates** (`df.filter()`) to reduce data load.\n",
    "  - Enable **AWS Glue Workflows** for dependency management.\n",
    "\n",
    "- **How do you troubleshoot a slow Glue job?**\n",
    "  - Check **Spark UI logs** for skewed partitions.\n",
    "  - Use `glueContext.getDataSink()` to track S3 write latency.\n",
    "  - Tune memory and executor settings.\n",
    "\n",
    "- **How do you design an ETL pipeline for real-time data ingestion?**\n",
    "  - Use **AWS Kinesis / Kafka** for ingestion.\n",
    "  - Process data with **Glue Streaming / Spark Structured Streaming**.\n",
    "  - Store transformed data in **S3 + Athena / Redshift** for querying.\n",
    "\n",
    "Would you like help with mock interviews or more scenario-based questions? ðŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
